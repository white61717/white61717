{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型思路\n",
    "\n",
    " 1. 將 content 當作第一句，answer 當作第二句，將兩者併在一起\n",
    " 3. label 用 pre_y 的方法將目標人名做出 NER array，省略其他非目標人名\n",
    " 4. 用 Bert+BiLSTM+CRF 模型，嘗試偵測 content 中 AML 人名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.csdn.net/qq_42933419/article/details/102973424?ops_request_misc=&request_id=&biz_id=102&utm_term=BERT%20%E5%95%8F%E7%AD%94&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-102973424\n",
    "#https://blog.csdn.net/guangyacyb/article/details/105526482?ops_request_misc=&request_id=&biz_id=102&utm_term=BERT%20%E5%95%8F%E7%AD%94&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-3-105526482\n",
    "#https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/#part-1-how-bert-is-applied-to-question-answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, Bidirectional, LSTM, TimeDistributed, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "from keras_bert import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "import codecs\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckiptagger import WS, POS, NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "bert_dir = r'C:\\Users\\rocker\\Python - deep learning\\Deep learning\\keras 大神\\bert'\n",
    "config_path = os.path.join(bert_dir, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(bert_dir, 'bert_model.ckpt')\n",
    "dict_path = os.path.join(bert_dir, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\rocker\\Python - deep learning\\Deep learning\\keras 大神\\content_df_4.csv')\n",
    "data = data[(data[\"status\"]==\"ok\")].drop([\"url\",\"context\",\"raw_content\",\"status\"],axis =1)\n",
    "#data = data[data['name'] != '[]']\n",
    "data['name'] = data['name'].apply(lambda x: x.strip().replace('[','').replace(']','').replace('\\'',''))\n",
    "#data['aml_label'] = data['name'].apply(lambda x: 0 if x == '[]' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.loc[~data['news_id'].isin([246, 1067, 1621, 1762, 2418, 2476, 2483, 2666, 2774, 3299,3144, 3676, 3898, 4064, 4597, 4659, 4950])]\n",
    "#1762 3676"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tag_dict(data):\n",
    "    y_token_dict = {}\n",
    "    for tokens in data['name']:\n",
    "        for token in tokens:\n",
    "            if token not in y_token_dict.keys():\n",
    "                y_token_dict[token] = len(y_token_dict)+1\n",
    "    #word_dict_reverse = {v: k for k, v in y_token_dict.items()}\n",
    "    #tag_tokenizer = Tokenizer(y_token_dict)\n",
    "    \n",
    "    return y_token_dict\n",
    "\n",
    "def create_tokenizer(dict_path):\n",
    "    token_dict = {}\n",
    "    with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
    "        for line in reader:\n",
    "            token = line.strip()\n",
    "            token_dict[token] = len(token_dict)\n",
    "    return token_dict\n",
    "\n",
    "def transfer(i):\n",
    "    if i != 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def encoded(tokenizer, data, question):\n",
    "    \n",
    "    x, y, z = [], [], []\n",
    "    for content in data['content']:\n",
    "        x1, x2 = tokenizer.encode(first=question, second=content, max_len=maxlen)\n",
    "        x3 = [transfer(i) for i in x1]\n",
    "        x.append(x1)\n",
    "        y.append(x2)\n",
    "        z.append(x3)\n",
    "    return x, y, z\n",
    "\n",
    "def tag_encoded(data, y_dict):\n",
    "    z = []\n",
    "    for tags in data['name']:\n",
    "        x = []\n",
    "        for tag in tags:\n",
    "            x1 = y_dict.get(tag)\n",
    "            x.append(x1)\n",
    "\n",
    "        z.append(x)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 400\n",
    "batch_size = 8\n",
    "epochs = 10\n",
    "input_shape = (maxlen, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '銀行為打擊犯罪訂定洗錢防制制度，哪些人是有涉及洗錢、詐騙等金融犯罪的高風險族群？'\n",
    "token_dict = create_tokenizer(dict_path)\n",
    "tokenizer = Tokenizer(token_dict)\n",
    "input_id, segment_id, mask_input = encoded(tokenizer, data, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_y(sentence, label, pad_num):\n",
    "    y = np.zeros((len(sentence),pad_num))\n",
    "    for i in range(len(sentence)):\n",
    "        sen = sentence[i][0:pad_num]\n",
    "        tokens = tokenizer.tokenize(first=sen)\n",
    "        if len(tokens) > pad_num:\n",
    "            tokens = tokens[0:pad_num-1]\n",
    "            tokens.append('[SEP]')\n",
    "        lab = label[i]\n",
    "        lab_y = np.zeros(len(tokens))\n",
    "        if lab!='':\n",
    "            top = None\n",
    "            #label只有一项\n",
    "            if ',' not in lab:\n",
    "                tokens_lab = tokenizer.tokenize(lab)\n",
    "                num_lab=len(tokens_lab)-2\n",
    "                for j in range(len(tokens)):\n",
    "                    if j+num_lab-1<len(tokens):\n",
    "                        if (tokens[j] == tokens_lab[1]) & (tokens[j + num_lab - 1] == tokens_lab[1 + num_lab - 1]):\n",
    "                            top = j\n",
    "                if top==None:\n",
    "                    print('error:' + sen)\n",
    "                    print(tokens,tokens_lab)\n",
    "                    break\n",
    "                lab_y[top] = 1  # B-COM\n",
    "                for g in range(top + 1, top + num_lab):\n",
    "                    lab_y[g] = 1  # I-COM\n",
    "            #label有多项\n",
    "            else:\n",
    "                t=[]\n",
    "                n=[]\n",
    "                for u in lab.split(','):\n",
    "                    tokens_lab = tokenizer.tokenize(u)\n",
    "                    num_lab = len(tokens_lab) - 2\n",
    "                    for j in range(len(tokens)):\n",
    "                        if j + num_lab - 1 < len(tokens):\n",
    "                            if (tokens[j] == tokens_lab[1]) & (tokens[j + num_lab - 1] == tokens_lab[1 + num_lab - 1]):\n",
    "                                t.append(j)\n",
    "                                n.append(num_lab)\n",
    "                if len(t)==0:\n",
    "                    print('error:' + sen)\n",
    "                    break\n",
    "                for t_num in range(len(t)):\n",
    "                    lab_y[t[t_num]] = 1  # B-COM\n",
    "                    for g in range(t[t_num]+1,t[t_num]+n[t_num]):\n",
    "                        lab_y[g] = 1  # I-COM\n",
    "            y[i]=np.lib.pad(lab_y, (0,pad_num-len(tokens)), 'constant', constant_values=(0,0))\n",
    "        else:\n",
    "            y[i][0] = 1    #若無答案則答案 = [CLS]\n",
    "\n",
    "    return y #y.reshape((len(sentence),pad_num,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pre_y(list(data['content']), list(data['name']), maxlen)\n",
    "label = label.reshape((label.shape[0], label.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_BiLSTM_CRF_model():\n",
    "    model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen)\n",
    "    bert_output = model.layers[-9].output\n",
    "    X = Lambda(lambda x: x[:, 0: input_shape[0]])(bert_output)\n",
    "    X = Bidirectional(LSTM(128, return_sequences=True))(X)\n",
    "    output = Dense(1, activation='softmax')(X)\n",
    "    model = Model(model.input, output)\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.layers[-1].trainable = True\n",
    "    model.layers[-2].trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = bert_BiLSTM_CRF_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, 400)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, 400)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 400, 768), ( 16226304    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 400, 768)     1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 400, 768)     0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 400, 768)     307200      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 400, 768)     0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 400, 768)     1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 400, 768)     2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 400, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 400, 768)     0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 400, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 400, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 400, 768)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 400, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 400, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 400, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 400, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 400, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 400, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 400, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 400, 768)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 400, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 400, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 400, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 400, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 400, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 400, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 400, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 400, 768)     0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 400, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 400, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 400, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 400, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 400, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 400, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 400, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 400, 768)     0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 400, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 400, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 400, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 400, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 400, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 400, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 400, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 400, 768)     0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 400, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 400, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 400, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 400, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 400, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 400, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 400, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 400, 768)     0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 400, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 400, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 400, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 400, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 400, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 400, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 400, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 400, 768)     0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 400, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 400, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 400, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 400, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 400, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 400, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 400, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 400, 768)     0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 400, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 400, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 400, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 400, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 400, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 400, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 400, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 400, 768)     0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 400, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 400, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 400, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 400, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 400, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 400, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 400, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 400, 768)     0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 400, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 400, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 400, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 400, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 400, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 400, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 400, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 400, 768)     0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 400, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 400, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 400, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 400, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 400, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 400, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 400, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 400, 768)     0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 400, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 400, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 400, 768)     0           Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 400, 256)     918528      lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 400, 1)       257         bidirectional_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 102,509,825\n",
      "Trainable params: 918,785\n",
      "Non-trainable params: 101,591,040\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps, warmup_steps = calc_train_steps(\n",
    "    num_example=data.shape[0],\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    warmup_proportion=0.1,\n",
    ")\n",
    "\n",
    "optimizer = AdamWarmup(total_steps, warmup_steps, lr=1e-3, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4387 samples, validate on 488 samples\n",
      "Epoch 1/10\n",
      "4387/4387 [==============================] - 282s 64ms/step - loss: 0.9960 - val_loss: 0.9962\n",
      "Epoch 2/10\n",
      "4387/4387 [==============================] - 284s 65ms/step - loss: 0.9960 - val_loss: 0.9962\n",
      "Epoch 3/10\n",
      "4387/4387 [==============================] - 277s 63ms/step - loss: 0.9960 - val_loss: 0.9962\n",
      "Epoch 4/10\n",
      "4387/4387 [==============================] - 277s 63ms/step - loss: 0.9960 - val_loss: 0.9962\n",
      "Epoch 5/10\n",
      "4387/4387 [==============================] - 281s 64ms/step - loss: 0.9960 - val_loss: 0.9962\n",
      "Epoch 6/10\n",
      "4387/4387 [==============================] - 282s 64ms/step - loss: 0.9960 - val_loss: 0.9962\n",
      "Epoch 7/10\n",
      "4387/4387 [==============================] - 284s 65ms/step - loss: 0.9960 - val_loss: 0.9962\n",
      "Epoch 8/10\n",
      "4387/4387 [==============================] - 284s 65ms/step - loss: 0.9960 - val_loss: 0.9962\n",
      "Epoch 9/10\n",
      "4387/4387 [==============================] - 281s 64ms/step - loss: 0.9960 - val_loss: 0.9962\n",
      "Epoch 10/10\n",
      "4387/4387 [==============================] - 280s 64ms/step - loss: 0.9960 - val_loss: 0.9962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2626ae4e408>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([input_id, segment_id, mask_input],\n",
    "          label,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_orig = '今年8月14日，張安樂因遭控收受政治獻金卻未依法申報，還涉嫌協助兒子張瑋成立的華夏大地旅行社逃漏稅、掏空公司資產2千多萬元，遭台北地檢署依違反《政治獻金法》、幫助逃漏稅及業務侵占等罪起訴，但莊子逵是無辜的，莊子逵沒有犯罪。由於該旅行社有港幣近千萬元的境外資金疑透過地下匯兌匯入，張家對資金來源交代不清，檢調懷疑背後可能是中資。沒想到檢調追查意外發現，包括張安樂在內的統促黨幹部近年積極透過各種管道滲透地方宮廟，掌握宮廟資源以發展該黨組織，懷疑統促黨很可能利用政府對宗教團體採高度自由管理的巧門，藉部分宮廟財務不透明的有利條件，淪為中資入台洗錢管道。傳出統促黨總裁「白狼」透過各種管道滲透全台30地方宮廟，恐淪中資入台的洗錢管道（圖／翻攝畫面）檢視相片傳出統促黨總裁「白狼」透過各種管道滲透全台30地方宮廟，恐淪中資入台的洗錢管道（圖／翻攝畫面）更多根據＜鏡周刊＞報導，統促黨幹部主事的宮廟已遍及多個縣市，包括新北市、彰化縣、雲林縣、台南市等地都有類似案例。就連已有3百多年歷史的知名寺廟雲林北港朝天宮也成為統促黨拉攏目標，不但該廟董事吳東合入黨，統促黨乾隆黨部也聘任該廟董事蔡上元擔任榮譽顧問。其中，張安樂本身就擔任雲林斗六五路財神宮榮譽董事長，還成立中華伏羲協會擔任會長，並由統促黨主席張馥堂擔任理事長，每年都舉辦兩岸共祭人文始祖伏羲典禮等兩岸宗教交流活動，'\n",
    "x1, x2 = tokenizer.encode(first=question, second=y_orig, max_len=maxlen)\n",
    "x3 = [transfer(i) for i in x1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict([[x1], [x2], [x3]])[0], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(maxlen):\n",
    "    print(word_dict.get(x1[i]), y_pred[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
