{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_starter = pd.read_csv('census_starter.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(census_starter, how='left', on='cfips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['last_month_data'] = train.groupby(['cfips'])['microbusiness_density'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['year'] = train.first_day_of_month.apply(lambda x: int(x.split('-')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['pct_bb_1'] = train.year.apply(lambda y: 'pct_bb_'+ str(y-2))\n",
    "train['pct_bb_2'] = train.year.apply(lambda y: 'pct_bb_'+ str(y-1))\n",
    "\n",
    "train['pct_college_1'] = train.year.apply(lambda y: 'pct_college_'+ str(y-2))\n",
    "train['pct_college_2'] = train.year.apply(lambda y: 'pct_college_'+ str(y-1))\n",
    "\n",
    "train['pct_foreign_born_1'] = train.year.apply(lambda y: 'pct_foreign_born_'+ str(y-2))\n",
    "train['pct_foreign_born_2'] = train.year.apply(lambda y: 'pct_foreign_born_'+ str(y-1))\n",
    "\n",
    "train['pct_it_workers_1'] = train.year.apply(lambda y: 'pct_it_workers_'+ str(y-2))\n",
    "train['pct_it_workers_2'] = train.year.apply(lambda y: 'pct_it_workers_'+ str(y-1))\n",
    "\n",
    "train['median_hh_inc_1'] = train.year.apply(lambda y: 'median_hh_inc_'+ str(y-2))\n",
    "train['median_hh_inc_2'] = train.year.apply(lambda y: 'median_hh_inc_'+ str(y-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pct_bb_1'] = train.apply(lambda x: x[x['pct_bb_1']], axis=1)\n",
    "train['pct_bb_2'] = train.apply(lambda x: x[x['pct_bb_2']], axis=1)\n",
    "\n",
    "train['pct_college_1'] = train.apply(lambda x: x[x['pct_college_1']], axis=1)\n",
    "train['pct_college_2'] = train.apply(lambda x: x[x['pct_college_2']], axis=1)\n",
    "\n",
    "train['pct_foreign_born_1'] = train.apply(lambda x: x[x['pct_foreign_born_1']], axis=1)\n",
    "train['pct_foreign_born_2'] = train.apply(lambda x: x[x['pct_foreign_born_2']], axis=1)\n",
    "\n",
    "train['pct_it_workers_1'] = train.apply(lambda x: x[x['pct_it_workers_1']], axis=1)\n",
    "train['pct_it_workers_2'] = train.apply(lambda x: x[x['pct_it_workers_2']], axis=1)\n",
    "\n",
    "train['median_hh_inc_1'] = train.apply(lambda x: x[x['median_hh_inc_1']], axis=1)\n",
    "train['median_hh_inc_2'] = train.apply(lambda x: x[x['median_hh_inc_2']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(census_starter.drop(['cfips'], axis=1), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[~train.last_month_data.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['microbusiness_density_precent'] = train.microbusiness_density/train.last_month_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_fcips = train[(train['microbusiness_density_precent'] > 2) | (train['microbusiness_density_precent'] < 0.5)][['cfips']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[~train['cfips'].isin(remove_fcips.cfips.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date1(train, n_steps, batch_size, shift):\n",
    "\n",
    "    x = None\n",
    "    # 先取前1500，再大我的電腦會kernal died\n",
    "    for i in train.cfips.unique()[:2]:\n",
    "#         print(i)\n",
    "        data = train[train.cfips == i][['last_month_data', 'pct_bb_1', 'pct_bb_2', 'pct_college_1', 'pct_college_2', 'pct_foreign_born_1', 'pct_foreign_born_2', 'pct_it_workers_1', 'pct_it_workers_2', 'median_hh_inc_1', 'median_hh_inc_2']]\n",
    "        y = train[train.cfips == i][['microbusiness_density_precent']].reset_index(drop=True)\n",
    "        \n",
    "        minmax = preprocessing.MinMaxScaler()\n",
    "        data = pd.DataFrame(minmax.fit_transform(data))\n",
    "        data['y'] = y\n",
    "        \n",
    "        dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "        window_length = n_steps + 10\n",
    "        dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "        dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "        if x == None:\n",
    "            x = dataset\n",
    "        else:\n",
    "            x = x.concatenate(dataset)\n",
    "\n",
    "    x = x.batch(batch_size)\n",
    "    x1 = x.map(lambda windows: (windows[:, :n_steps, 0:1], windows[:, :n_steps, 1:-1], windows[:, -10:, -1]))\n",
    "#     x2 = x.map(lambda windows: (windows[:, :n_steps, 1:-1]))\n",
    "#     y = x.map(lambda windows: (windows[:, -10:, -1]))\n",
    "    \n",
    "    \n",
    "\n",
    "    x1 = x1.shuffle(10000, seed=1)\n",
    "#     x2 = x2.shuffle(10000, seed=1)\n",
    "#     y = y.shuffle(10000, seed=1)\n",
    "    \n",
    "#     for a, b, c in zip(x1, x2, y):\n",
    "#         stack_x1 = np.stack(a.numpy()) \n",
    "#         stack_x2 = np.stack(b.numpy()) \n",
    "#         stack_y = np.stack(c.numpy())  \n",
    "    for a, b, c in x1:\n",
    "        stack_x1 = np.stack(a.numpy()) \n",
    "        stack_x2 = np.stack(b.numpy()) \n",
    "        stack_y = np.stack(c.numpy())  \n",
    "#     x1 = x1.prefetch(1)\n",
    "#     x2 = x2.prefetch(1)\n",
    "#     x1 = x.map(lambda windows: (windows[:, :n_steps, :-1], windows[:, -10:, -1]))\n",
    "    return stack_x1, stack_x2, stack_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, y = prepare_date1(train, n_steps=20, batch_size=64, shift=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_avg_init = tf.keras.initializers.VarianceScaling(scale=2, mode='fan_avg', distribution='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.losses import MeanAbsolutePercentageError\n",
    "from tensorflow.keras.layers import Dense, Lambda, concatenate, Bidirectional, LSTM, TimeDistributed, BatchNormalization, Concatenate\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(None, 1))\n",
    "inputs_opendata_reshape = tf.reshape(Input(shape=(None, 10)), [-1, 10])\n",
    "\n",
    "x = Bidirectional(LSTM(64, return_sequences=True))(inputs)\n",
    "x = LayerNormalization()(x)\n",
    "x = Bidirectional(LSTM(64, return_sequences=False))(x)\n",
    "x = Dense(64, activation='elu', kernel_initializer=he_avg_init)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x_concat = Concatenate(axis=-1)([x, inputs_opendata_reshape])\n",
    "x_concat = Dense(32, activation='elu', kernel_initializer=he_avg_init)(x_concat)\n",
    "x_concat = Dense(10)(x_concat)\n",
    "model = Model(inputs=[inputs, inputs_opendata_reshape], outputs=x_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=MeanAbsolutePercentageError(), optimizer=\"adam\")\n",
    "model.compile(loss='mse', optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_dataset = tf.data.Dataset.zip((x,y))\n",
    "# combined_dataset = combined_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.from_tensors(((a,b), c))\n",
    "# model.fit(dataset, epochs=10, steps_per_epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_29 (InputLayer)          [(None, None, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " bidirectional_24 (Bidirectiona  (None, None, 128)   33792       ['input_29[0][0]']               \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, None, 128)   256         ['bidirectional_24[1][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " bidirectional_25 (Bidirectiona  (None, 128)         98816       ['layer_normalization_12[1][0]'] \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 64)           8256        ['bidirectional_25[1][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 64)          256         ['dense_27[1][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " input_31 (InputLayer)          [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 74)           0           ['batch_normalization_10[1][0]', \n",
      "                                                                  'input_31[0][0]']               \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 32)           2400        ['concatenate_10[1][0]']         \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 10)           330         ['dense_28[1][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 144,106\n",
      "Trainable params: 143,978\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\losses.py\", line 1486, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 20 and 10 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](model_8/dense_29/BiasAdd, Cast)' with input shapes: [20,10], [10,1].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m dataset_label \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(y)\n\u001b[0;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip((dataset_12, dataset_label))\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileoufx3wl2.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\rocker\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\losses.py\", line 1486, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 20 and 10 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](model_8/dense_29/BiasAdd, Cast)' with input shapes: [20,10], [10,1].\n"
     ]
    }
   ],
   "source": [
    "dataset_12 = tf.data.Dataset.from_tensor_slices((x1, x2))\n",
    "dataset_label = tf.data.Dataset.from_tensor_slices(y)\n",
    "\n",
    "dataset = tf.data.Dataset.zip((dataset_12, dataset_label))\n",
    "model.fit(dataset, epochs=10, steps_per_epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 20, 1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generator():\n",
    "#     for s1, s2, l in zip(x1, x2, y):\n",
    "#         yield {\"input_1\": s1, \"input_2\": s2}, l\n",
    "\n",
    "# dataset = tf.data.Dataset.from_generator(generator, output_types=({\"input_5\": tf.float64, \"input_6\": tf.float64}, tf.float64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _input_fn():\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\": x1, \"input_2\": x2}, y))\n",
    "  dataset = dataset.batch(2, drop_remainder=True)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = tf.data.Dataset.from_tensors(x1)\n",
    "# b = tf.data.Dataset.from_tensors(x2)\n",
    "# c = tf.data.Dataset.from_tensors(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.from_tensor_slices({\"input_1\": x1, \"input_2\": x2}, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# history = model.fit(_input_fn() , epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
